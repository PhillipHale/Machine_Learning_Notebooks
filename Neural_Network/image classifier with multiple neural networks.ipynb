{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classificaiton \n",
    "\n",
    "In the notebook we will walkthrough an end-to-end image classification. The objective is to take a set of images and classify them into different categoreis based on object oimages. The categories used are as follows:\n",
    "* Landscapes\n",
    "* City Scapes\n",
    "* Food\n",
    "* Concerts\n",
    "* Group Photos\n",
    "* My Face\n",
    "\n",
    "\n",
    "Each category will have around 100 images which we will developed a training model to learn various classification. \n",
    "\n",
    "We will be using the Convolutional Neural Network for the classification. \n",
    "\n",
    "Big thanks to [DarkBones](https://github.com/DarkBones/CNN-Image-Classifier/) the datasets and source code. Here is the [narative](https://github.com/DarkBones/CNN-Image-Classifier/blob/master/Capstone%20Proposal.pdf) by DarkBones. \n",
    "\n",
    "Without futher a-do, lets get started!\n",
    "\n",
    "# Step 1: Image Processing\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redne/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import re\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.join('..', 'application', 'images' )\n",
    "originals_dir = os.path.join(root_dir, 'original')\n",
    "training_dir = os.path.join(root_dir, 'train')\n",
    "val_dir = os.path.join(root_dir, 'validation')\n",
    "test_dir = os.path.join(root_dir, 'test')\n",
    "\n",
    "target_imagesize = (256, 256)\n",
    "\n",
    "# size of the test and validation sets as compared to the total amount of images\n",
    "test_size = 0.2\n",
    "validation_size = 0.2\n",
    "\n",
    "clear_existing_data = False \n",
    "# if true, data in training, test and validation directories will be deleted before splitting the \n",
    "# .. the data in the original direcories\n",
    "\n",
    "random_seed = 7\n",
    "\n",
    "# the amount of images in the training, validation and test sets\n",
    "training_count = 0\n",
    "validation_count = 0\n",
    "test_count = 0\n",
    "\n",
    "# list of categories\n",
    "categories = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 1: Split the dataset\n",
    "\n",
    "A model will be training with the training set and validated iwh the validation set. __Whenever the perfomrance on the validation set improves the weights oof the model are saved__ _(overwriting the previously best performing weights)_. Several models will be training in this fassion. \n",
    "\n",
    "AFter all models have been trainined, we choose the one that performs the best on data it hasnt seen before: __the test set.__\n",
    "\n",
    "This is why, in order to train our model and test how well the model is performing, the dataset must be split up randomly into ___ training, validation and test sets__.\n",
    "\n",
    "The `__split_dataset` funciton will go through the images in the original dataset and split them into training, validation and test sets according to the test_size and validation_size parameters set above. \n",
    "\n",
    "Both functions can only be called from inside the class, since the plublic `initalize()` function will call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove all files in given directory \n",
    "def __empty_directory(path):\n",
    "    for file in os.listdir(path):\n",
    "        os.remove(os.path.join(path, file))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits images in original directory into training, test and validation directories\n",
    "def __split_dataset():\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    size_count = 0\n",
    "    for category in os.listdir(originals_dir):\n",
    "        # make a new directory where they don't exist and empty existing directories\n",
    "        for p in [re.split(r'[\\\\/]', training_dir)[-1], re.split(r'[\\\\/]', val_dir)[-1], re.split(r'[\\\\/]', test_dir)[-1]]:\n",
    "            if not os.path.exists(os.path.join(root_dir, p, category)):\n",
    "                os.makedirs(os.path.join(root_dir, p, category))\n",
    "            if clear_existing_data == True:\n",
    "                __empty_directory(os.path.join(root_dir, p, category))\n",
    "            \n",
    "        # collect all the files in the originals directory\n",
    "        files = []\n",
    "        for file in os.listdir(os.path.join(originals_dir, category)):\n",
    "            files.append(file)\n",
    "        \n",
    "        # calculate the training, validation and test set sizes\n",
    "        test_count = round(len(files) * test_size)\n",
    "        validation_count = round(len(files) * val_size)\n",
    "        train_count = len(files) - test_count - validation_count\n",
    "        \n",
    "        # randomly shuffle the array of files\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        for i, file in enumerate(files):\n",
    "            location = None\n",
    "            if i < test_count:\n",
    "                location = test_dir\n",
    "            elif i < test_count + validation_count:\n",
    "                location = val_dir\n",
    "            else:\n",
    "                location = training_dir\n",
    "                \n",
    "            shutil.copyfile(os.path.join(originals_dir, category, file), os.path.join(location, category, file))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2: Getting additional class parameters\n",
    "\n",
    "`ImagePreprpcessor` class described in this notebook may need to know additional information with regards to how large each dataset is and what the names of the categories are. \n",
    "\n",
    "The `training_count`, `validation_count`, `test_count`, and `categories` variables are not assigned any values by default. After the data has been split, we can count how many images are in each of the datasets and what categories are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an array with the category names\n",
    "def __get_categories():\n",
    "    return [item[len(originals_dir)+1:] for item in sorted(glob(os.path.join(originals_dir, \"*\")))]\n",
    "\n",
    "# returns the sizes of the training, validation and test sets\n",
    "def __get_dataset_sizes():\n",
    "    train_size = sum([len(files) for r, d, files in os.walk(training_dir)])\n",
    "    validation_size = sum([len(files) for r, d, files in os.walk(val_dir)])\n",
    "    test_size = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "    \n",
    "    return train_size, validation_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3: Initialize the class\n",
    "Now that all private functions are in place, we can call the public master function to call them in the correct order.\n",
    "\n",
    "The `intialize` function is the only funcitno that needs to be called after inailizing the `ImagePreProcessor` class. It will preprocess to split the datasets and the class will store information about the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(self):\n",
    "    __split_dataset()\n",
    "    \n",
    "    training_count, validation_count, test_count = __get_dataset_sizes()\n",
    "    categories = __get_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Convert images into tensors\n",
    "\n",
    "The code sectio below converts these images into ___tensors;___ matrices of numeric values representing how bright each pixel in the image is. The numeric values are then normalized so they are all within a range of between 0 and 1, rather than between 0 and 255. \n",
    "\n",
    "Then __normaliztion__ makes it easier for the model to train, as all pixels are now in the same range relative to the brightest pixel in each particular image. The brightest pixel in a dark image will still be of value 1, even though it may not have been 255 before the normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a list of image filepaths and retunr a lost of 4D tensors\n",
    "def file_to_tensor(self, file):\n",
    "    img = image.load_img(file, target_size=target_imagesize)\n",
    "    x = image.img_to_array(img)\n",
    "    x *= (1.0/x.max()) # set the range of the tensor values between 0 and 1\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def files_to_tensors(self, files):\n",
    "    list_of_tensors = [file_to_tensor(file) for file in files]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Load datasets\n",
    "The function below takes a directory and returns a list of image locations, along with the list of one-hot encoded targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file locations and labels\n",
    "def load_dataset(path):\n",
    "    data = load_fileS(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), max(data['terget'])+1)\n",
    "    return files, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Choosing a Model\n",
    "\n",
    "This is a continuaiton from the pervious notebook __1_image_preprocessing.__ In This section we will slect how different models are trained and how we can determine which of the models achieved the best results. \n",
    "\n",
    "The models we will training include:\n",
    "* neuralnet_model\n",
    "* cnn_model\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "\n",
    "All modes that are trained will be saved in a list. AFter training we will evaluate the performance of the modes from the persepctive for the `F1 Score`. Thereafter, we use the testing set (which the model shavent seen before) to calculate their F1 Score. \n",
    "\n",
    "___Key Objective: The model with the hightest F1 score on the testing set, is the model we're going to use.___ \n",
    "\n",
    "\n",
    "Big thanks to [DarkBones](https://github.com/DarkBones/CNN-Image-Classifier/) the datasets and source code. Here is the [narative](https://github.com/DarkBones/CNN-Image-Classifier/blob/master/Capstone%20Proposal.pdf) by DarkBones. \n",
    "\n",
    "\n",
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_preprocessor import ImagePreprocessor\n",
    "## import models\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "## kares key imports\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.join('..', 'application', 'images')\n",
    "originals_dir = os.path.join(root_dir, \"original\")\n",
    "training_dir = os.path.join(root_dir, \"train\")\n",
    "test_dir = os.path.join(root_dir, \"test\")\n",
    "val_dir = os.path.join(root_dir, \"validation\")\n",
    "\n",
    "target_imagesize = (256, 256)\n",
    "\n",
    "clear_existing_data = True # if true, data in training, test and validation directories will be deleted before splitting the data in the originals directory\n",
    "augment_data = True # whether images should be augmented during preprocessing\n",
    "augmentations = 20 # how many augmentations to make for each original image\n",
    "\n",
    "random_seed = 7\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "saved_models_dir = os.path.join('..', 'application', 'saved_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 image categories\n",
      "464 total images\n",
      "\n",
      "278 training images\n",
      "93 validation images\n",
      "93 test images\n",
      "\n",
      "Categories:\n",
      "  - animal\n",
      "  - city_scape\n",
      "  - food\n",
      "  - group\n",
      "  - landscape\n",
      "  - me\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ImagePreprocessor()\n",
    "preprocessor.root_dir = root_dir\n",
    "preprocessor.originals_dir = originals_dir\n",
    "preprocessor.training_dir = training_dir\n",
    "preprocessor.test_dir = test_dir\n",
    "preprocessor.val_dir = val_dir\n",
    "preprocessor.random_seed = random_seed\n",
    "preprocessor.target_imagesize = target_imagesize\n",
    "preprocessor.clear_existing_data = clear_existing_data\n",
    "\n",
    "preprocessor.initialize()\n",
    "categories = preprocessor.categories\n",
    "training_count = preprocessor.training_count\n",
    "validation_count = preprocessor.validation_count\n",
    "test_count = preprocessor.test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__get_categories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-54533c0df4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m__get_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '__get_categories' is not defined"
     ]
    }
   ],
   "source": [
    "__get_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate augmented images\n",
    "\n",
    "bc our dataset is relatively small (only a few hundred images), we will artificailly generate more data by augmenting the images. The original images are randomly rotated, zoomed and / or flipped horizontally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 278 images belonging to 7 classes.\n",
      "Found 93 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "img_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255,\n",
    "    fill_mode='reflect')\n",
    "\n",
    "train_generator = img_datagen.flow_from_directory(training_dir,\n",
    "                                                   target_size=target_imagesize,\n",
    "                                                   batch_size=augmentations,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=random_seed)\n",
    "\n",
    "validation_generator = img_datagen.flow_from_directory(val_dir,\n",
    "                                                   target_size=target_imagesize,\n",
    "                                                   batch_size=augmentations,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model: Random guessing\n",
    "\n",
    "To get a better understanding of how well our model work, we will first create a model that just randomly guesses the categories and calculates its F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_guesses(count):\n",
    "    guesses = []\n",
    "    for i in range(count):\n",
    "        guesses.append(random.randint(0, len(categories)))\n",
    "    return guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the F1 score\n",
    "\n",
    "To measure how good the model is at predicting the correct category for images, we calculate the model's F1 score. The higher this score, the better the model is performing. IN the ned, we choose the model that has the highest F1 score on the images in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(img_path, pred_model):\n",
    "    img_tensor = preprocessor.file_to_tensor(img_path)\n",
    "    try:\n",
    "        h = pred_model.predict(img_tensor)\n",
    "    except:\n",
    "        img_tensor = img_tensor.reshape(1,x_length)\n",
    "        h = pred_model.predict(img_tensor)\n",
    "    return categories[np.argmax(h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_cal(model=None):\n",
    "    test_images = np.array(glob(os.path.join(test_dir, \"*\", \"*\")))\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for img in test_images:\n",
    "        y_true.append(categories.index(re.split(r'[\\\\/]',img)[-2]))\n",
    "        if model != None:\n",
    "            pred = predict_category(img, model)\n",
    "            y_pred.append(categories.index(pred))\n",
    "        \n",
    "    if model == None:\n",
    "        y_pred = random_guesses(len(y_true))\n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will store the training models in a list called `models`, along with another list of the model names so we can erfer to them later. After all modesl are trained, we will calculate their `f1 scores` and store them in the `f1_scores` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "modelnames = []\n",
    "f1_scores = []\n",
    "\n",
    "models.append(None)\n",
    "modelnames.append(\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model: Traditional Neural Network\n",
    "\n",
    "To test the effectiveness of Convolutional Neural Networks, we will also create a `traditional Feed-Froward Neural Network` to see how it compares against `CNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redne/anaconda3/lib/python3.6/site-packages/PIL/Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    }
   ],
   "source": [
    "# load the file locations and their labels 'y'\n",
    "train_files, y_train = preprocessor.load_dataset(training_dir)\n",
    "val_files, y_val = preprocessor.load_dataset(val_dir)\n",
    "test_files, y_test = preprocessor.load_dataset(test_dir)\n",
    "\n",
    "# load training, validation, and test matrices\n",
    "x_train = preprocessor.files_to_tensors(train_files)\n",
    "x_val = preprocessor.files_to_tensors(val_files)\n",
    "x_test = preprocessor.files_to_tensors(test_files)\n",
    "\n",
    "# calculate how many pixels are in the images\n",
    "x_length = 1\n",
    "for n in x_train.shape[1:]:\n",
    "    x_length *= n\n",
    "\n",
    "# reshape the tensors into single dimensions\n",
    "x_train = x_train.reshape(len(x_train),x_length)\n",
    "x_val = x_val.reshape(len(x_val),x_length)\n",
    "x_test = x_test.reshape(len(x_test),x_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               100663808 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 101,192,711\n",
      "Trainable params: 101,192,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neuralnet_model = Sequential()\n",
    "neuralnet_model.add(Dense(512, input_dim=x_length, activation='relu'))\n",
    "neuralnet_model.add(Dense(512, activation='relu'))\n",
    "neuralnet_model.add(Dense(512, activation='relu'))\n",
    "neuralnet_model.add(Dropout(0.3))\n",
    "neuralnet_model.add(Dense(len(categories)+1, activation='softmax'))\n",
    "neuralnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278 samples, validate on 93 samples\n",
      "Epoch 1/10\n",
      "278/278 [==============================] - 6s 23ms/step - loss: 2.0038 - acc: 0.1511 - val_loss: 1.7580 - val_acc: 0.3656\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.75800, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n",
      "Epoch 2/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1.7455 - acc: 0.3058 - val_loss: 1.6488 - val_acc: 0.2796\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.75800 to 1.64878, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n",
      "Epoch 3/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1.6515 - acc: 0.3489 - val_loss: 1.5880 - val_acc: 0.3978\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.64878 to 1.58802, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n",
      "Epoch 4/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1.5594 - acc: 0.3957 - val_loss: 1.5216 - val_acc: 0.3978\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.58802 to 1.52161, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n",
      "Epoch 5/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1.4725 - acc: 0.4281 - val_loss: 1.4624 - val_acc: 0.4409\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.52161 to 1.46238, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n",
      "Epoch 6/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1.3837 - acc: 0.4964 - val_loss: 1.4136 - val_acc: 0.4516\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.46238 to 1.41358, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n",
      "Epoch 7/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1.3481 - acc: 0.5108 - val_loss: 1.3809 - val_acc: 0.4946\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.41358 to 1.38090, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n",
      "Epoch 8/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1.2799 - acc: 0.5468 - val_loss: 1.3570 - val_acc: 0.4731\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.38090 to 1.35705, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n",
      "Epoch 9/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1.2405 - acc: 0.5540 - val_loss: 1.3527 - val_acc: 0.5161\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.35705 to 1.35269, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n",
      "Epoch 10/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1.1585 - acc: 0.6115 - val_loss: 1.3311 - val_acc: 0.5269\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.35269 to 1.33107, saving model to ../application/saved_models/traditional_neuralnet.hdf5\n"
     ]
    }
   ],
   "source": [
    "neuralnet_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=os.path.join(saved_models_dir, 'traditional_neuralnet.hdf5'), \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "neuralnet_model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer])\n",
    "\n",
    "neuralnet_model.load_weights(filepath=os.path.join(saved_models_dir,'traditional_neuralnet.hdf5'))\n",
    "models.append(neuralnet_model)\n",
    "modelnames.append(\"benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a new CNN Model from scratch\n",
    "\n",
    "in teh code below, we will create a new Convolutional Neural Network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 256, 256, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 128, 128, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 64, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               16777728  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 17,087,415\n",
      "Trainable params: 17,087,415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(filters=16,\n",
    "                     kernel_size=2, \n",
    "                     strides=(1, 1), \n",
    "                     padding='same', \n",
    "                     activation='relu', \n",
    "                     input_shape=(target_imagesize[0], target_imagesize[1], 3)))\n",
    "cnn_model.add(MaxPooling2D(pool_size=2))\n",
    "cnn_model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=2))\n",
    "cnn_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=2))\n",
    "cnn_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=2))\n",
    "cnn_model.add(Dropout(0.3))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(512, activation='relu'))\n",
    "cnn_model.add(Dropout(0.3))\n",
    "cnn_model.add(Dense(512, activation='relu'))\n",
    "cnn_model.add(Dropout(0.3))\n",
    "cnn_model.add(Dense(len(categories)+1, activation='softmax'))\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  7/278 [..............................] - ETA: 8:12 - loss: 1.7971 - acc: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redne/anaconda3/lib/python3.6/site-packages/PIL/Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/278 [==============================] - 577s 2s/step - loss: 1.7717 - acc: 0.2552 - val_loss: 1.7288 - val_acc: 0.2924\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.72880, saving model to ../application/saved_models/cnn_from_scratch.hdf5\n",
      "Epoch 2/3\n",
      "278/278 [==============================] - 579s 2s/step - loss: 1.7465 - acc: 0.2676 - val_loss: 1.7056 - val_acc: 0.3276\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.72880 to 1.70563, saving model to ../application/saved_models/cnn_from_scratch.hdf5\n",
      "Epoch 3/3\n",
      "278/278 [==============================] - 575s 2s/step - loss: 1.7267 - acc: 0.2829 - val_loss: 1.6811 - val_acc: 0.3287\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.70563 to 1.68106, saving model to ../application/saved_models/cnn_from_scratch.hdf5\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=os.path.join(saved_models_dir,'cnn_from_scratch.hdf5'), \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "cnn_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])\n",
    "cnn_model.fit_generator(train_generator,\n",
    "                        steps_per_epoch=training_count, \n",
    "                        #epochs=epochs,\n",
    "                        epochs = 3,\n",
    "                        validation_data = validation_generator,\n",
    "                        validation_steps=validation_count,\n",
    "                        callbacks=[checkpointer],\n",
    "                        verbose=1)\n",
    "\n",
    "cnn_model.load_weights(filepath=os.path.join(saved_models_dir,'cnn_from_scratch.hdf5'))\n",
    "models.append(cnn_model)\n",
    "modelnames.append(\"new cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "__Transfer Learning__ is taking an existing, pre trained model and stripping off the final layer and replacing it with our won layers so it can classify the objects we'er looking for. We will use three different pre-trained netwokrs to see how they compare:\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = []\n",
    "base_models.append(VGG19(include_top=False, \n",
    "                        weights = 'imagenet',\n",
    "                        input_shape = (target_imagesize[0], target_imagesize[1], 3)))\n",
    "base_models.append(ResNet50(include_top=False,\n",
    "                           weights = 'imagenet',\n",
    "                           input_shape = (target_imagesize[0], target_imagesize[1], 3)))\n",
    "base_models.append(InceptionV3(include_top=False,\n",
    "                              weights = 'imagenet', \n",
    "                              input_shape = (target_imagesize[0], target_imagesize[1], 3)))\n",
    "\n",
    "base_modelnames = \"VGG19 ResNet50 InceptionV3\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Models: ResNet50\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redne/anaconda3/lib/python3.6/site-packages/PIL/Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/27 [=>............................] - ETA: 8:29 - loss: 2.1483 - acc: 0.0806"
     ]
    }
   ],
   "source": [
    "#train the models\n",
    "for i, model in enumerate(base_models):\n",
    "    print(\"\")\n",
    "    print(\"Training Models: %s\" % base_modelnames[1])\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(saved_models_dir,\n",
    "                                                        base_modelnames[i] + '.hdf5'))\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # custom layers\n",
    "    cus_layers = model.output\n",
    "    cus_layers = Flatten()(cus_layers)\n",
    "    cus_layers = Dense(1024, activation='relu')(cus_layers)\n",
    "    cus_layers = Dropout(0.3)(cus_layers)\n",
    "    cus_layers = Dense(512, activation='relu')(cus_layers)\n",
    "    cus_layers = Dropout(0.3)(cus_layers)\n",
    "    cus_layers = Dense(len(categories)+1, activation='relu')(cus_layers)\n",
    "    predictions = Dense(len(categories)+1, activation='softmax')(cus_layers)\n",
    "    \n",
    "    # create the final model\n",
    "    model_final = Model(inputs = model.input, outputs = predictions)\n",
    "    \n",
    "    # compile the model\n",
    "    model_final.compile(loss='categorical_crossentropy', optimizer = optimizers.SGD(lr=0.0001,\n",
    "                                                                                   momentum=0.9),\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    model_final.fit_generator(train_generator, \n",
    "                             steps_per_epoch = training_count // 10, \n",
    "                             epochs = 3, \n",
    "                             validation_data = validation_generator, \n",
    "                             validation_steps = validation_count // 10,\n",
    "                             callbacks=[checkpointer],\n",
    "                             verbose=1)\n",
    "    \n",
    "    model_final.load_weights(finalpath=os.path.join(saved_models_dir, base_modelnames[i] + '.hdf5'))\n",
    "    \n",
    "    models.append(model_final)\n",
    "    modelnames.append(base_modelnames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the model\n",
    "Now that all our models ar etrained, we can calculate their F1 scores on the test set. Important is that none of the models have seen data from the test set during the training phase, so it will be good indicaiton of how th emodel will perform in a real-world scenarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "# load the file locations and their labels 'y'\n",
    "test_files, y_test = preprocessor.load_dataset(test_dir)\n",
    "\n",
    "# load training, validation, and test matrices\n",
    "x_test = preprocessor.files_to_tensors(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    f1score = f1_score_cal(model)\n",
    "    f1_scores.append(f1score)\n",
    "    print(modelnames[i], f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(6)\n",
    "\n",
    "rects1 = ax.bar(index, f1_scores)\n",
    "\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Scores per model')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(modelnames)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Knowing that a higher F1 score is better, we can see that the `InceptionV3` model is outperforming all the other models with an `F1 score of 0.74`. We will take the InceptionV3 model and fine tune in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
